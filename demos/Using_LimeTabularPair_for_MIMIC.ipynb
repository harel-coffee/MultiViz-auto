{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIMIC_Multimodal_LIME.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXjPKtbwrGvr",
        "outputId": "3c558616-5325-4f70-c918-34a788fd0842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools as it\n",
        "import numpy as np\n",
        "import pickle\n",
        "import sys\n",
        "import torch\n",
        "sys.path.insert(0, '/content/drive/MyDrive/MMA')\n",
        "sys.path.insert(0, '/content/drive/MyDrive/MMA/MultiBench')"
      ],
      "metadata": {
        "id": "YR9VQXfOrIXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install memory-profiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "subT4QjBrNHU",
        "outputId": "1f5fc9a8-f63d-4037-aa9d-146d5656b910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting memory-profiler\n",
            "  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler) (5.4.8)\n",
            "Building wheels for collected packages: memory-profiler\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=5554d2be9851ed5c6797c969c70f34e1703c4328f5719ed2dfa1e39a3d9c939f\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n",
            "Successfully built memory-profiler\n",
            "Installing collected packages: memory-profiler\n",
            "Successfully installed memory-profiler-0.60.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct7P-_Q7rOaJ",
        "outputId": "2b03f330-f42c-4113-f498-debf023b1a98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 1.4 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20 kB 2.7 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 71 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 81 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 275 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from lime) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lime) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lime) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from lime) (4.62.3)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from lime) (1.0.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime) (0.18.3)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (7.1.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2021.11.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (3.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->lime) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->lime) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->lime) (3.1.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283857 sha256=1e95b9d23277aa0ccfdb27f5e998cf1c9ca68c12a541e8813954c14e53c8b825\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/cb/e5/ac701e12d365a08917bf4c6171c0961bc880a8181359c66aa7\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unimodals.common_models import MLP, GRU # noqa\n",
        "from datasets.mimic.get_data import get_dataloader # noqa\n",
        "from fusions.common_fusions import Concat # noqa\n",
        "from training_structures.Supervised_Learning import train, test # noqa\n",
        "\n",
        "import lime"
      ],
      "metadata": {
        "id": "1bZW-P8trPt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data sanity check\n",
        "\n",
        "fp = '/content/drive/MyDrive/MMA/im.pk'\n",
        "with open(fp, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "print(data.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXJ3wZeNrSAg",
        "outputId": "2cb3b4af-d90b-4512-8446-ee36861aa8a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['temp', 'ep_tdata', 'adm_features_all', 'adm_labels_all', 'y_icd9'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "traindata, validdata, testdata = get_dataloader(7, imputed_path=fp, train_shuffle=False)\n",
        "\n",
        "# build encoders, head and fusion layer\n",
        "# encoders = [MLP(5, 10, 10, dropout=False).cuda(), GRU(\n",
        "#     12, 30, dropout=False, batch_first=True).cuda()]\n",
        "# head = MLP(730, 40, 2, dropout=False).cuda()\n",
        "# fusion = Concat().cuda()\n",
        "\n",
        "# train\n",
        "# train(encoders, fusion, head, traindata, validdata, 20, auprc=True)\n",
        "# !cp best.pt /content/drive/MyDrive/MMA/saved_models/mimic_baseline.pt\n",
        "\n",
        "model = torch.load('/content/drive/MyDrive/MMA/saved_models/mimic_baseline.pt').cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhHbei31rTcs",
        "outputId": "8a35e396-041f-40b4-9033-e5a3073721be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = None\n",
        "train_y = None\n",
        "\n",
        "for j in traindata:\n",
        "    if train_X != None:\n",
        "        train_X = torch.cat([train_X, torch.cat([j[0], j[1].reshape(j[1].shape[0], -1)], dim=1)], dim=0)\n",
        "        train_y = torch.cat([train_y, j[2]], dim=0)\n",
        "    else: \n",
        "        train_X = torch.cat([j[0], j[1].reshape(j[1].shape[0], -1)], dim=1)\n",
        "        train_y = j[2]\n",
        "\n",
        "train_X = train_X.float().numpy()\n",
        "train_y = train_y.float().numpy()"
      ],
      "metadata": {
        "id": "7SQsocD6rU9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "namelist = ['age', 'acquired immunodeficiency syndrome', 'hematologic malignancy', 'metastatic cancer',\n",
        "'admission type', 'glasgow coma scale', 'systolic blood pressure', 'heart rate', 'body temperature',\n",
        "'pao2 / fio2 ratio', 'urine output', 'serum urea nitrogen level', 'white blood cells count',\n",
        "'serum bicarbonate level', 'sodium level', 'potassium level', 'bilirubin level']\n",
        "\n",
        "static_names = namelist[:5]\n",
        "timeseries_names = namelist[5:]\n",
        "idx_to_name = {}\n",
        "\n",
        "cnt = 0\n",
        "for i in range(len(static_names)):\n",
        "    for j in range(len(timeseries_names)):\n",
        "        for k in range(24):\n",
        "            idx_to_name[cnt] = f'{static_names[i]}_{timeseries_names[j]}_{k}'\n",
        "            cnt += 1"
      ],
      "metadata": {
        "id": "unsgctNv3l4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clf_fn(X):\n",
        "    X = torch.from_numpy(X).cuda().float()\n",
        "    static = X[:, :5]\n",
        "    timeseries = X[:, 5:].reshape(X.shape[0], 24, 12)\n",
        "    logits = model([static, timeseries])\n",
        "    return torch.softmax(logits, 1).cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "SWkiVw2crWUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import copy\n",
        "from functools import partial\n",
        "import json\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import sklearn\n",
        "import sklearn.preprocessing\n",
        "from sklearn.utils import check_random_state\n",
        "# from pyDOE2 import lhs\n",
        "from scipy.stats.distributions import norm\n",
        "\n",
        "from lime.discretize import QuartileDiscretizer\n",
        "from lime.discretize import DecileDiscretizer\n",
        "from lime.discretize import EntropyDiscretizer\n",
        "from lime.discretize import BaseDiscretizer\n",
        "from lime.discretize import StatsDiscretizer\n",
        "from lime import explanation\n",
        "from lime import lime_base"
      ],
      "metadata": {
        "id": "BDJ_8NgPsE1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LimeTabularExplainer(object):\n",
        "    \"\"\"Explains predictions on tabular (i.e. matrix) data.\n",
        "    For numerical features, perturb them by sampling from a Normal(0,1) and\n",
        "    doing the inverse operation of mean-centering and scaling, according to the\n",
        "    means and stds in the training data. For categorical features, perturb by\n",
        "    sampling according to the training distribution, and making a binary\n",
        "    feature that is 1 when the value is the same as the instance being\n",
        "    explained.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 training_data,\n",
        "                 mode=\"classification\",\n",
        "                 training_labels=None,\n",
        "                 feature_names=None,\n",
        "                 categorical_features=None,\n",
        "                 categorical_names=None,\n",
        "                 kernel_width=None,\n",
        "                 kernel=None,\n",
        "                 verbose=False,\n",
        "                 class_names=None,\n",
        "                 feature_selection='auto',\n",
        "                 discretize_continuous=True,\n",
        "                 discretizer='quartile',\n",
        "                 sample_around_instance=False,\n",
        "                 random_state=None,\n",
        "                 training_data_stats=None):\n",
        "        \"\"\"Init function.\n",
        "        Args:\n",
        "            training_data: numpy 2d array\n",
        "            mode: \"classification\" or \"regression\"\n",
        "            training_labels: labels for training data. Not required, but may be\n",
        "                used by discretizer.\n",
        "            feature_names: list of names (strings) corresponding to the columns\n",
        "                in the training data.\n",
        "            categorical_features: list of indices (ints) corresponding to the\n",
        "                categorical columns. Everything else will be considered\n",
        "                continuous. Values in these columns MUST be integers.\n",
        "            categorical_names: map from int to list of names, where\n",
        "                categorical_names[x][y] represents the name of the yth value of\n",
        "                column x.\n",
        "            kernel_width: kernel width for the exponential kernel.\n",
        "                If None, defaults to sqrt (number of columns) * 0.75\n",
        "            kernel: similarity kernel that takes euclidean distances and kernel\n",
        "                width as input and outputs weights in (0,1). If None, defaults to\n",
        "                an exponential kernel.\n",
        "            verbose: if true, print local prediction values from linear model\n",
        "            class_names: list of class names, ordered according to whatever the\n",
        "                classifier is using. If not present, class names will be '0',\n",
        "                '1', ...\n",
        "            feature_selection: feature selection method. can be\n",
        "                'forward_selection', 'lasso_path', 'none' or 'auto'.\n",
        "                See function 'explain_instance_with_data' in lime_base.py for\n",
        "                details on what each of the options does.\n",
        "            discretize_continuous: if True, all non-categorical features will\n",
        "                be discretized into quartiles.\n",
        "            discretizer: only matters if discretize_continuous is True\n",
        "                and data is not sparse. Options are 'quartile', 'decile',\n",
        "                'entropy' or a BaseDiscretizer instance.\n",
        "            sample_around_instance: if True, will sample continuous features\n",
        "                in perturbed samples from a normal centered at the instance\n",
        "                being explained. Otherwise, the normal is centered on the mean\n",
        "                of the feature data.\n",
        "            random_state: an integer or numpy.RandomState that will be used to\n",
        "                generate random numbers. If None, the random state will be\n",
        "                initialized using the internal numpy seed.\n",
        "            training_data_stats: a dict object having the details of training data\n",
        "                statistics. If None, training data information will be used, only matters\n",
        "                if discretize_continuous is True. Must have the following keys:\n",
        "                means\", \"mins\", \"maxs\", \"stds\", \"feature_values\",\n",
        "                \"feature_frequencies\"\n",
        "        \"\"\"\n",
        "        self.random_state = check_random_state(random_state)\n",
        "        self.mode = mode\n",
        "        self.categorical_names = categorical_names or {}\n",
        "        self.sample_around_instance = sample_around_instance\n",
        "        self.training_data_stats = training_data_stats\n",
        "\n",
        "        # Check and raise proper error in stats are supplied in non-descritized path\n",
        "        if self.training_data_stats:\n",
        "            self.validate_training_data_stats(self.training_data_stats)\n",
        "\n",
        "        if categorical_features is None:\n",
        "            categorical_features = []\n",
        "        if feature_names is None:\n",
        "            feature_names = [str(i) for i in range(training_data.shape[1])]\n",
        "\n",
        "        self.categorical_features = list(categorical_features)\n",
        "        self.feature_names = list(feature_names)\n",
        "\n",
        "        self.discretizer = None\n",
        "        if discretize_continuous and not sp.sparse.issparse(training_data):\n",
        "            # Set the discretizer if training data stats are provided\n",
        "            if self.training_data_stats:\n",
        "                discretizer = StatsDiscretizer(\n",
        "                    training_data, self.categorical_features,\n",
        "                    self.feature_names, labels=training_labels,\n",
        "                    data_stats=self.training_data_stats,\n",
        "                    random_state=self.random_state)\n",
        "\n",
        "            if discretizer == 'quartile':\n",
        "                self.discretizer = QuartileDiscretizer(\n",
        "                        training_data, self.categorical_features,\n",
        "                        self.feature_names, labels=training_labels,\n",
        "                        random_state=self.random_state)\n",
        "            elif discretizer == 'decile':\n",
        "                self.discretizer = DecileDiscretizer(\n",
        "                        training_data, self.categorical_features,\n",
        "                        self.feature_names, labels=training_labels,\n",
        "                        random_state=self.random_state)\n",
        "            elif discretizer == 'entropy':\n",
        "                self.discretizer = EntropyDiscretizer(\n",
        "                        training_data, self.categorical_features,\n",
        "                        self.feature_names, labels=training_labels,\n",
        "                        random_state=self.random_state)\n",
        "            elif isinstance(discretizer, BaseDiscretizer):\n",
        "                self.discretizer = discretizer\n",
        "            else:\n",
        "                raise ValueError('''Discretizer must be 'quartile',''' +\n",
        "                                 ''' 'decile', 'entropy' or a''' +\n",
        "                                 ''' BaseDiscretizer instance''')\n",
        "            self.categorical_features = list(range(training_data.shape[1]))\n",
        "            self.pair_features = list(range(5*288))\n",
        "\n",
        "            # Get the discretized_training_data when the stats are not provided\n",
        "            if(self.training_data_stats is None):\n",
        "                discretized_training_data = self.discretizer.discretize(\n",
        "                    training_data)\n",
        "\n",
        "        if kernel_width is None:\n",
        "            kernel_width = np.sqrt(training_data.shape[1]) * .75\n",
        "        kernel_width = float(kernel_width)\n",
        "\n",
        "        if kernel is None:\n",
        "            def kernel(d, kernel_width):\n",
        "                return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
        "\n",
        "        kernel_fn = partial(kernel, kernel_width=kernel_width)\n",
        "\n",
        "        self.feature_selection = feature_selection\n",
        "        self.base = lime_base.LimeBase(kernel_fn, verbose, random_state=self.random_state)\n",
        "        self.class_names = class_names\n",
        "\n",
        "        # Though set has no role to play if training data stats are provided\n",
        "        self.scaler = sklearn.preprocessing.StandardScaler(with_mean=False)\n",
        "        self.scaler.fit(training_data)\n",
        "        self.feature_values = {}\n",
        "        self.feature_frequencies = {}\n",
        "\n",
        "        for feature in self.categorical_features:\n",
        "            if training_data_stats is None:\n",
        "                if self.discretizer is not None:\n",
        "                    column = discretized_training_data[:, feature]\n",
        "                else:\n",
        "                    column = training_data[:, feature]\n",
        "\n",
        "                feature_count = collections.Counter(column)\n",
        "                values, frequencies = map(list, zip(*(sorted(feature_count.items()))))\n",
        "            else:\n",
        "                values = training_data_stats[\"feature_values\"][feature]\n",
        "                frequencies = training_data_stats[\"feature_frequencies\"][feature]\n",
        "\n",
        "            self.feature_values[feature] = values\n",
        "            self.feature_frequencies[feature] = (np.array(frequencies) /\n",
        "                                                 float(sum(frequencies)))\n",
        "            self.scaler.mean_[feature] = 0\n",
        "            self.scaler.scale_[feature] = 1\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_and_round(values):\n",
        "        return ['%.2f' % v for v in values]\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_training_data_stats(training_data_stats):\n",
        "        \"\"\"\n",
        "            Method to validate the structure of training data stats\n",
        "        \"\"\"\n",
        "        stat_keys = list(training_data_stats.keys())\n",
        "        valid_stat_keys = [\"means\", \"mins\", \"maxs\", \"stds\", \"feature_values\", \"feature_frequencies\"]\n",
        "        missing_keys = list(set(valid_stat_keys) - set(stat_keys))\n",
        "        if len(missing_keys) > 0:\n",
        "            raise Exception(\"Missing keys in training_data_stats. Details: %s\" % (missing_keys))\n",
        "\n",
        "    def explain_instance(self,\n",
        "                         data_row,\n",
        "                         predict_fn,\n",
        "                         labels=(1,),\n",
        "                         top_labels=None,\n",
        "                         num_features=10,\n",
        "                         num_samples=5000,\n",
        "                         distance_metric='euclidean',\n",
        "                         model_regressor=None,\n",
        "                         sampling_method='gaussian'):\n",
        "        \"\"\"Generates explanations for a prediction.\n",
        "        First, we generate neighborhood data by randomly perturbing features\n",
        "        from the instance (see __data_inverse). We then learn locally weighted\n",
        "        linear models on this neighborhood data to explain each of the classes\n",
        "        in an interpretable way (see lime_base.py).\n",
        "        Args:\n",
        "            data_row: 1d numpy array or scipy.sparse matrix, corresponding to a row\n",
        "            predict_fn: prediction function. For classifiers, this should be a\n",
        "                function that takes a numpy array and outputs prediction\n",
        "                probabilities. For regressors, this takes a numpy array and\n",
        "                returns the predictions. For ScikitClassifiers, this is\n",
        "                `classifier.predict_proba()`. For ScikitRegressors, this\n",
        "                is `regressor.predict()`. The prediction function needs to work\n",
        "                on multiple feature vectors (the vectors randomly perturbed\n",
        "                from the data_row).\n",
        "            labels: iterable with labels to be explained.\n",
        "            top_labels: if not None, ignore labels and produce explanations for\n",
        "                the K labels with highest prediction probabilities, where K is\n",
        "                this parameter.\n",
        "            num_features: maximum number of features present in explanation\n",
        "            num_samples: size of the neighborhood to learn the linear model\n",
        "            distance_metric: the distance metric to use for weights.\n",
        "            model_regressor: sklearn regressor to use in explanation. Defaults\n",
        "                to Ridge regression in LimeBase. Must have model_regressor.coef_\n",
        "                and 'sample_weight' as a parameter to model_regressor.fit()\n",
        "            sampling_method: Method to sample synthetic data. Defaults to Gaussian\n",
        "                sampling. Can also use Latin Hypercube Sampling.\n",
        "        Returns:\n",
        "            An Explanation object (see explanation.py) with the corresponding\n",
        "            explanations.\n",
        "        \"\"\"\n",
        "        if sp.sparse.issparse(data_row) and not sp.sparse.isspmatrix_csr(data_row):\n",
        "            # Preventative code: if sparse, convert to csr format if not in csr format already\n",
        "            data_row = data_row.tocsr()\n",
        "        data, inverse = self.__data_inverse(data_row, num_samples, sampling_method)\n",
        "        # if sp.sparse.issparse(data):\n",
        "        #     # Note in sparse case we don't subtract mean since data would become dense\n",
        "        #     scaled_data = data.multiply(self.scaler.scale_)\n",
        "        #     # Multiplying with csr matrix can return a coo sparse matrix\n",
        "        #     if not sp.sparse.isspmatrix_csr(scaled_data):\n",
        "        #         scaled_data = scaled_data.tocsr()\n",
        "        # else:\n",
        "        #     scaled_data = (data - self.scaler.mean_) / self.scaler.scale_\n",
        "        scaled_data = data.copy()\n",
        "        distances = sklearn.metrics.pairwise_distances(\n",
        "                scaled_data,\n",
        "                scaled_data[0].reshape(1, -1),\n",
        "                metric=distance_metric\n",
        "        ).ravel()\n",
        "\n",
        "        yss = predict_fn(inverse)\n",
        "\n",
        "        # for classification, the model needs to provide a list of tuples - classes\n",
        "        # along with prediction probabilities\n",
        "        if self.mode == \"classification\":\n",
        "            if len(yss.shape) == 1:\n",
        "                raise NotImplementedError(\"LIME does not currently support \"\n",
        "                                          \"classifier models without probability \"\n",
        "                                          \"scores. If this conflicts with your \"\n",
        "                                          \"use case, please let us know: \"\n",
        "                                          \"https://github.com/datascienceinc/lime/issues/16\")\n",
        "            elif len(yss.shape) == 2:\n",
        "                if self.class_names is None:\n",
        "                    self.class_names = [str(x) for x in range(yss[0].shape[0])]\n",
        "                else:\n",
        "                    self.class_names = list(self.class_names)\n",
        "                if not np.allclose(yss.sum(axis=1), 1.0):\n",
        "                    warnings.warn(\"\"\"\n",
        "                    Prediction probabilties do not sum to 1, and\n",
        "                    thus does not constitute a probability space.\n",
        "                    Check that you classifier outputs probabilities\n",
        "                    (Not log probabilities, or actual class predictions).\n",
        "                    \"\"\")\n",
        "            else:\n",
        "                raise ValueError(\"Your model outputs \"\n",
        "                                 \"arrays with {} dimensions\".format(len(yss.shape)))\n",
        "\n",
        "        # for regression, the output should be a one-dimensional array of predictions\n",
        "        else:\n",
        "            try:\n",
        "                if len(yss.shape) != 1 and len(yss[0].shape) == 1:\n",
        "                    yss = np.array([v[0] for v in yss])\n",
        "                assert isinstance(yss, np.ndarray) and len(yss.shape) == 1\n",
        "            except AssertionError:\n",
        "                raise ValueError(\"Your model needs to output single-dimensional \\\n",
        "                    numpyarrays, not arrays of {} dimensions\".format(yss.shape))\n",
        "\n",
        "            predicted_value = yss[0]\n",
        "            min_y = min(yss)\n",
        "            max_y = max(yss)\n",
        "\n",
        "            # add a dimension to be compatible with downstream machinery\n",
        "            yss = yss[:, np.newaxis]\n",
        "\n",
        "        feature_names = copy.deepcopy(self.feature_names)\n",
        "        if feature_names is None:\n",
        "            feature_names = [str(x) for x in range(data_row.shape[0])]\n",
        "\n",
        "        if sp.sparse.issparse(data_row):\n",
        "            values = self.convert_and_round(data_row.data)\n",
        "            feature_indexes = data_row.indices\n",
        "        else:\n",
        "            values = self.convert_and_round(data_row)\n",
        "            feature_indexes = None\n",
        "\n",
        "        for i in self.categorical_features:\n",
        "            if self.discretizer is not None and i in self.discretizer.lambdas:\n",
        "                continue\n",
        "            name = int(data_row[i])\n",
        "            if i in self.categorical_names:\n",
        "                name = self.categorical_names[i][name]\n",
        "            feature_names[i] = '%s=%s' % (feature_names[i], name)\n",
        "            values[i] = 'True'\n",
        "        categorical_features = self.categorical_features\n",
        "\n",
        "        discretized_feature_names = None\n",
        "        if self.discretizer is not None:\n",
        "            categorical_features = range(data.shape[1])\n",
        "            discretized_instance = self.discretizer.discretize(data_row)\n",
        "            discretized_feature_names = copy.deepcopy(feature_names)\n",
        "            for f in self.discretizer.names:\n",
        "                discretized_feature_names[f] = self.discretizer.names[f][int(\n",
        "                        discretized_instance[f])]\n",
        "\n",
        "        # domain_mapper = TableDomainMapper(feature_names,\n",
        "        #                                   values,\n",
        "        #                                   scaled_data[0],\n",
        "        #                                   categorical_features=categorical_features,\n",
        "        #                                   discretized_feature_names=discretized_feature_names,\n",
        "        #                                   feature_indexes=feature_indexes)\n",
        "        # ret_exp = explanation.Explanation(domain_mapper,\n",
        "        #                                   mode=self.mode,\n",
        "        #                                   class_names=self.class_names)\n",
        "        # if self.mode == \"classification\":\n",
        "        #     ret_exp.predict_proba = yss[0]\n",
        "        #     if top_labels:\n",
        "        #         labels = np.argsort(yss[0])[-top_labels:]\n",
        "        #         ret_exp.top_labels = list(labels)\n",
        "        #         ret_exp.top_labels.reverse()\n",
        "        # else:\n",
        "        #     ret_exp.predicted_value = predicted_value\n",
        "        #     ret_exp.min_value = min_y\n",
        "        #     ret_exp.max_value = max_y\n",
        "        #     labels = [0]\n",
        "\n",
        "        ret_exp = {'intercept': {},\n",
        "                   'local_exp': {},\n",
        "                   'score': {},\n",
        "                   'local_pred': {}}\n",
        "        \n",
        "        for label in labels:\n",
        "            (ret_exp['intercept'][label],\n",
        "             ret_exp['local_exp'][label],\n",
        "             ret_exp['score'][label],\n",
        "             ret_exp['local_pred'][label]) = self.base.explain_instance_with_data(\n",
        "                    scaled_data,\n",
        "                    yss,\n",
        "                    distances,\n",
        "                    label,\n",
        "                    num_features,\n",
        "                    model_regressor=model_regressor,\n",
        "                    feature_selection=self.feature_selection)\n",
        "\n",
        "        # if self.mode == \"regression\":\n",
        "        #     ret_exp.intercept[1] = ret_exp.intercept[0]\n",
        "        #     ret_exp.local_exp[1] = [x for x in ret_exp.local_exp[0]]\n",
        "        #     ret_exp.local_exp[0] = [(i, -1 * j) for i, j in ret_exp.local_exp[1]]\n",
        "\n",
        "        return ret_exp\n",
        "\n",
        "    def data_to_pair_data(self, masks):\n",
        "        static = masks[:, :5]\n",
        "        timeseries = masks[:, 5:]\n",
        "        pair_masks = np.asarray([i*j for i in static.T for j in timeseries.T]).T\n",
        "        return pair_masks\n",
        "\n",
        "    def __data_inverse(self,\n",
        "                       data_row,\n",
        "                       num_samples,\n",
        "                       sampling_method):\n",
        "        \"\"\"Generates a neighborhood around a prediction.\n",
        "        For numerical features, perturb them by sampling from a Normal(0,1) and\n",
        "        doing the inverse operation of mean-centering and scaling, according to\n",
        "        the means and stds in the training data. For categorical features,\n",
        "        perturb by sampling according to the training distribution, and making\n",
        "        a binary feature that is 1 when the value is the same as the instance\n",
        "        being explained.\n",
        "        Args:\n",
        "            data_row: 1d numpy array, corresponding to a row\n",
        "            num_samples: size of the neighborhood to learn the linear model\n",
        "            sampling_method: 'gaussian' or 'lhs'\n",
        "        Returns:\n",
        "            A tuple (data, inverse), where:\n",
        "                data: dense num_samples * K matrix, where categorical features\n",
        "                are encoded with either 0 (not equal to the corresponding value\n",
        "                in data_row) or 1. The first row is the original instance.\n",
        "                inverse: same as data, except the categorical features are not\n",
        "                binary, but categorical (as the original data)\n",
        "        \"\"\"\n",
        "        is_sparse = sp.sparse.issparse(data_row)\n",
        "        if is_sparse:\n",
        "            num_cols = data_row.shape[1]\n",
        "            data = sp.sparse.csr_matrix((num_samples, num_cols), dtype=data_row.dtype)\n",
        "        else:\n",
        "            num_cols = data_row.shape[0]\n",
        "            data = np.zeros((num_samples, num_cols))\n",
        "        categorical_features = range(num_cols)\n",
        "        if self.discretizer is None:\n",
        "            instance_sample = data_row\n",
        "            scale = self.scaler.scale_\n",
        "            mean = self.scaler.mean_\n",
        "            if is_sparse:\n",
        "                # Perturb only the non-zero values\n",
        "                non_zero_indexes = data_row.nonzero()[1]\n",
        "                num_cols = len(non_zero_indexes)\n",
        "                instance_sample = data_row[:, non_zero_indexes]\n",
        "                scale = scale[non_zero_indexes]\n",
        "                mean = mean[non_zero_indexes]\n",
        "\n",
        "            if sampling_method == 'gaussian':\n",
        "                data = self.random_state.normal(0, 1, num_samples * num_cols\n",
        "                                                ).reshape(num_samples, num_cols)\n",
        "                data = np.array(data)\n",
        "            # elif sampling_method == 'lhs':\n",
        "            #     data = lhs(num_cols, samples=num_samples\n",
        "            #                ).reshape(num_samples, num_cols)\n",
        "            #     means = np.zeros(num_cols)\n",
        "            #     stdvs = np.array([1]*num_cols)\n",
        "            #     for i in range(num_cols):\n",
        "            #         data[:, i] = norm(loc=means[i], scale=stdvs[i]).ppf(data[:, i])\n",
        "            #     data = np.array(data)\n",
        "            else:\n",
        "                warnings.warn('''Invalid input for sampling_method.\n",
        "                                 Defaulting to Gaussian sampling.''', UserWarning)\n",
        "                data = self.random_state.normal(0, 1, num_samples * num_cols\n",
        "                                                ).reshape(num_samples, num_cols)\n",
        "                data = np.array(data)\n",
        "\n",
        "            if self.sample_around_instance:\n",
        "                data = data * scale + instance_sample\n",
        "            else:\n",
        "                data = data * scale + mean\n",
        "            if is_sparse:\n",
        "                if num_cols == 0:\n",
        "                    data = sp.sparse.csr_matrix((num_samples,\n",
        "                                                 data_row.shape[1]),\n",
        "                                                dtype=data_row.dtype)\n",
        "                else:\n",
        "                    indexes = np.tile(non_zero_indexes, num_samples)\n",
        "                    indptr = np.array(\n",
        "                        range(0, len(non_zero_indexes) * (num_samples + 1),\n",
        "                              len(non_zero_indexes)))\n",
        "                    data_1d_shape = data.shape[0] * data.shape[1]\n",
        "                    data_1d = data.reshape(data_1d_shape)\n",
        "                    data = sp.sparse.csr_matrix(\n",
        "                        (data_1d, indexes, indptr),\n",
        "                        shape=(num_samples, data_row.shape[1]))\n",
        "            categorical_features = self.categorical_features\n",
        "            first_row = data_row\n",
        "        else:\n",
        "            first_row = self.discretizer.discretize(data_row)\n",
        "        data[0] = data_row.copy()\n",
        "        inverse = data.copy()\n",
        "        for column in categorical_features:\n",
        "            values = self.feature_values[column]\n",
        "            freqs = self.feature_frequencies[column]\n",
        "            inverse_column = self.random_state.choice(values, size=num_samples,\n",
        "                                                      replace=True, p=freqs)\n",
        "            binary_column = (inverse_column == first_row[column]).astype(int)\n",
        "            binary_column[0] = 1\n",
        "            inverse_column[0] = data[0, column]\n",
        "            data[:, column] = binary_column\n",
        "            inverse[:, column] = inverse_column\n",
        "        if self.discretizer is not None:\n",
        "            inverse[1:] = self.discretizer.undiscretize(inverse[1:])\n",
        "        inverse[0] = data_row\n",
        "\n",
        "        data = self.data_to_pair_data(data)\n",
        "\n",
        "        return data, inverse"
      ],
      "metadata": {
        "id": "6OQQEoS0rYaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = LimeTabularExplainer(train_X.copy(), discretize_continuous=True, random_state=0)"
      ],
      "metadata": {
        "id": "bcXPCxeLr2v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 733\n",
        "sample = train_X[idx].copy()\n",
        "exp = explainer.explain_instance(sample, clf_fn, num_features=5, num_samples=50000)\n",
        "multimodal_exp = {idx_to_name[k]: v for k, v in exp['local_exp'][1]}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIEwfDAur7wA",
        "outputId": "0303ac2a-415b-4d24-b27d-55f1d52dc773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 50000, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multimodal_exp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5ILLCTU_9E4",
        "outputId": "1468714c-b209-40ab-c80c-37356e7d2ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acquired immunodeficiency syndrome_glasgow coma scale_10': 0.027062007232925542,\n",
              " 'acquired immunodeficiency syndrome_pao2 / fio2 ratio_16': 0.03137376454235384,\n",
              " 'acquired immunodeficiency syndrome_urine output_4': 0.028831172399166997,\n",
              " 'admission type_glasgow coma scale_0': 0.043610240876812735,\n",
              " 'age_glasgow coma scale_0': 0.04628793105763634}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 962\n",
        "sample = train_X[idx].copy()\n",
        "exp = explainer.explain_instance(sample, clf_fn, num_features=5, num_samples=50000)\n",
        "multimodal_exp = {idx_to_name[k]: v for k, v in exp['local_exp'][1]}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBkyA-gABGay",
        "outputId": "f12b7048-0a04-40ad-d14f-6ef48e4bc3e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 50000, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multimodal_exp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKsVmQ-jBYTI",
        "outputId": "318b71ce-a7af-498f-c653-0a968d599b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acquired immunodeficiency syndrome_potassium level_16': -0.04884980985662822,\n",
              " 'acquired immunodeficiency syndrome_sodium level_4': -0.04978007819279831,\n",
              " 'hematologic malignancy_potassium level_4': -0.04748055134868414,\n",
              " 'hematologic malignancy_serum bicarbonate level_16': -0.05098543421140426,\n",
              " 'hematologic malignancy_sodium level_16': -0.061339260293006063}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k9_jX9fCTBE",
        "outputId": "d949ed55-0f09-4022-aa8f-8bb467cb5452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intercept': {1: 0.7210279753951161},\n",
              " 'local_exp': {1: [(808, -0.061339260293006063),\n",
              "   (784, -0.05098543421140426),\n",
              "   (508, -0.04978007819279831),\n",
              "   (544, -0.04884980985662822),\n",
              "   (820, -0.04748055134868414)]},\n",
              " 'local_pred': {1: array([0.46259284])},\n",
              " 'score': {1: 0.14020653800964256}}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hQ0qdyOOJk2s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}